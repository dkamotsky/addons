{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discriminative Layer Training.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/image_ops\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/_template.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/_template.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/docs/tutorials/image_ops.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "[Update button links]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial will demonstrate how to implement discriminative layer training and how it can help in transfer learning. \n",
        "\n",
        "In this example, we will fine tune a pretrained imagenet resnet50 to classify a subset of the cifar 100 dataset, a tanks vs trains dataset. \n",
        "\n",
        "This tutorial will demonstrate that discriminative layer training helps improves training speed. The intuition is that lower layers are more generalizable and should be preserved, while higher layers are task specific. Setting a lower learning rate for the lower layers helps preserve general features for use by the high layers and prevent over fitting. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#it will be much faster on gpu, but you can still run this on cpu \n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ALfZ9Q37Ugn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --no-deps tensorflow-addons~=0.7\n",
        "!pip install typeguard\n",
        "\n",
        "#discriminative wrapper not available in current tfa\n",
        "!git clone https://github.com/hyang0129/addons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8Tf1Xo9VRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#duct taping to get the imports \n",
        "#will be changed or removed once we can import the wrapper from the main tfa modules\n",
        "\n",
        "import shutil \n",
        "\n",
        "shutil.copy(\"addons/tensorflow_addons/optimizers/discriminative_layer_training.py\", \"discriminative_layer_training.py\")\n",
        "\n",
        "from discriminative_layer_training import DiscriminativeWrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iwuzaUFIWuT",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs_KxybYIkwJ",
        "colab_type": "text"
      },
      "source": [
        "First, we want to prepare our dataset. We will download cifar 100 and only keep data in label 85 and 90 (tanks and trains) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKn-6_Hs90i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage import io \n",
        "import numpy as np \n",
        "\n",
        "\n",
        "train, test = tf.keras.datasets.cifar100.load_data()\n",
        "\n",
        "#find the tanks and trains and filter down the dataset\n",
        "train_tanksandtrains = np.isin(train[1], [85, 90]).flatten()\n",
        "\n",
        "train_x = train[0][train_tanksandtrains ]\n",
        "train_y = train[1][train_tanksandtrains ]\n",
        "#if is tank then 1 else 0 \n",
        "train_y = (train_y == 85) * 1\n",
        "\n",
        "# do the same for test dataset\n",
        "test_tanksandtrains = np.isin(test[1], [85, 90]).flatten()\n",
        "\n",
        "test_x = test[0][test_tanksandtrains] \n",
        "test_y = test[1][test_tanksandtrains] \n",
        "test_y = (test_y == 85) * 1\n",
        "\n",
        "\n",
        "# show a train \n",
        "print(train_y[0])\n",
        "io.imshow(train_x[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fAulgMLi1R",
        "colab_type": "text"
      },
      "source": [
        "We will also use some data augmentation because our training set is very small (1k images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUqGtS-jLhR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#create a data generator for augmentation \n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "#we only have 1000 training images, so we limit the steps to ensure the generator doesn't run out \n",
        "epochs = 10 \n",
        "steps = 1000//64\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SofAMdvCLgtP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvJ5X6YVLbtu",
        "colab_type": "text"
      },
      "source": [
        "##Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Lk1WYzI9kb",
        "colab_type": "text"
      },
      "source": [
        "This is our model function. It is a simple resnet50 with a pooling layer as the output. This gets fed to our classifer head. We will initialize this for regular training than reinitialize for discriminative layer training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpHgkPVBAjRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build a simple pretrained resnet with a custom head \n",
        "def get_model(): \n",
        "  model = tf.keras.Sequential() \n",
        "  model.add(tf.keras.applications.resnet50.ResNet50(weights = 'imagenet', \n",
        "                                                  input_shape = (32,32,3),\n",
        "                                                  include_top = False, \n",
        "                                                  pooling = 'avg'))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "  return model \n",
        "\n",
        "example_model = get_model()\n",
        "example_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLjgxrtULp22",
        "colab_type": "text"
      },
      "source": [
        "##Training Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPklCqBYJfrp",
        "colab_type": "text"
      },
      "source": [
        "This is regular training. We assign a learning rate for the whole model then train for 10 epochs. However, because Adam is a momentum based optimizer, it has a tendency to pick up on irrelevant low level features and overfit the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXVl1R-e9gw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get a copy of the model before any training\n",
        "model = get_model() \n",
        "\n",
        "#define optimizer and compile \n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = opt)\n",
        "\n",
        "#fit for 10 epochs\n",
        "model.fit(datagen.flow(train_x, train_y, batch_size=64), \n",
        "          steps_per_epoch = steps, \n",
        "          epochs = epochs, \n",
        "          validation_data= (test_x, test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeiIv-bMJ0vc",
        "colab_type": "text"
      },
      "source": [
        "Now we will attempt to correct that behaviour. We know that the lower level features don't need to be changed greatly, so we assign a lower learning rate multiplier of 0.1. If the overall learning rate is 0.001, then the resnet lower layers will learn at 0.1 * 0.001 = 0.0001, which is slower than the head. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQs0j6eVB2mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get a copy of the model before any training\n",
        "model = get_model() \n",
        "\n",
        "\"\"\"\n",
        "intuitively, the lower layers contain general features like shapes, etc \n",
        "these features shouldn't need to change drastically for this new task \n",
        "\"\"\"\n",
        "\n",
        "#assign layer 0, which is the resnet50 model an lr_mult of 0.1 to reduce lr \n",
        "model.layers[0].lr_mult = 0.1\n",
        "\n",
        "'''\n",
        "use the wrapper around an Adam class (do not pass an instance)\n",
        "you can pass other kwargs to the wrapper, they will go straight to the \n",
        "base_optimizer. This is because the wrapper creates a copy of the base_optimizer\n",
        "for each unique learning rate multiplier \n",
        "'''\n",
        "opt = DiscriminativeWrapper(base_optimizer = tf.keras.optimizers.Adam, \n",
        "                            model = model, \n",
        "                            learning_rate = 0.001, )\n",
        "\n",
        "#compile in the same way as a regular model \n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = opt)\n",
        "\n",
        "#fit in the same way as a regular model\n",
        "model.fit(datagen.flow(train_x, train_y, batch_size=64), \n",
        "          steps_per_epoch = steps, \n",
        "          epochs = epochs, \n",
        "          validation_data= (test_x, test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_ruHnAsKKwq",
        "colab_type": "text"
      },
      "source": [
        "Based on the results, you can see that slowing down the lower layers can help in transfer learning. This method requires more hyper parameter tuning, but can save you a lot of time for transfer learning tasks. By lowering the learning rate for lower layers, you can preserve the more generalizable features and allow your model to generalize better. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caAItxLmMttp",
        "colab_type": "text"
      },
      "source": [
        "I hope you find this tutorial helpful and find awesome ways to apply transfer learning and discriminative layer learning. "
      ]
    }
  ]
}